{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16edc39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3014c906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "mnist_data = tfds.load('fashion_mnist')\n",
    "for item in mnist_data:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac143d",
   "metadata": {},
   "source": [
    "딕셔너리값 trian, test 출력\n",
    "\n",
    "그러므로 load 할 때 split을 사용해 따로 가져올 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9349a8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.prefetch_op._PrefetchDataset'>\n"
     ]
    }
   ],
   "source": [
    "mnist_train = tfds.load('fashion_mnist', split='train')\n",
    "assert isinstance(mnist_train, tf.data.Dataset) # assert는 오른쪽 조건이 맞는지 아닌지 확인(아니면 동작x)\n",
    "print(type(mnist_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e5714",
   "metadata": {},
   "source": [
    "결과로 PrefetchDataset의 객체가 출력된다. 이 객체를 순회하면서 데이터 조사할 수 있다.\n",
    "\n",
    "이 클래스의 좋은 기능 하나는 take(1) 메서드를 호출해 첫 번째 레코드를 가져올 수 있다는 것이다.\n",
    "\n",
    "데이터가 어떻게 들어있는지 확인해보겠다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44609294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['image', 'label'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 14:20:56.284759: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for item in mnist_train.take(1):\n",
    "    print(type(item))\n",
    "    print(item.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca92a30",
   "metadata": {},
   "source": [
    "1. 첫 번쨰 print 문의 출력은 각 레코드의 타입이 딕셔너리라는 것을 보여준다.\n",
    "\n",
    "2. 이 딕셔너리의 키를 출력하면 image와 label임을 알 수 있다.\n",
    "\n",
    "따라서 데이터 값을 확인하려면 다음과 같이 쓸 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe5c8d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['image', 'label'])\n",
      "tf.Tensor(\n",
      "[[[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 18]\n",
      "  [ 77]\n",
      "  [227]\n",
      "  [227]\n",
      "  [208]\n",
      "  [210]\n",
      "  [225]\n",
      "  [216]\n",
      "  [ 85]\n",
      "  [ 32]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 61]\n",
      "  [100]\n",
      "  [ 97]\n",
      "  [ 80]\n",
      "  [ 57]\n",
      "  [117]\n",
      "  [227]\n",
      "  [238]\n",
      "  [115]\n",
      "  [ 49]\n",
      "  [ 78]\n",
      "  [106]\n",
      "  [108]\n",
      "  [ 71]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 81]\n",
      "  [105]\n",
      "  [ 80]\n",
      "  [ 69]\n",
      "  [ 72]\n",
      "  [ 64]\n",
      "  [ 44]\n",
      "  [ 21]\n",
      "  [ 13]\n",
      "  [ 44]\n",
      "  [ 69]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 80]\n",
      "  [114]\n",
      "  [ 80]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 26]\n",
      "  [ 92]\n",
      "  [ 69]\n",
      "  [ 68]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 71]\n",
      "  [ 74]\n",
      "  [ 83]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 78]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 83]\n",
      "  [ 77]\n",
      "  [108]\n",
      "  [ 34]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 55]\n",
      "  [ 92]\n",
      "  [ 69]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 71]\n",
      "  [ 71]\n",
      "  [ 77]\n",
      "  [ 69]\n",
      "  [ 66]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 80]\n",
      "  [ 80]\n",
      "  [ 78]\n",
      "  [ 94]\n",
      "  [ 63]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 63]\n",
      "  [ 95]\n",
      "  [ 66]\n",
      "  [ 68]\n",
      "  [ 72]\n",
      "  [ 72]\n",
      "  [ 69]\n",
      "  [ 72]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 80]\n",
      "  [ 77]\n",
      "  [106]\n",
      "  [ 61]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 80]\n",
      "  [108]\n",
      "  [ 71]\n",
      "  [ 69]\n",
      "  [ 72]\n",
      "  [ 71]\n",
      "  [ 69]\n",
      "  [ 72]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 72]\n",
      "  [ 72]\n",
      "  [ 75]\n",
      "  [ 78]\n",
      "  [ 72]\n",
      "  [ 85]\n",
      "  [128]\n",
      "  [ 64]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 88]\n",
      "  [120]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 72]\n",
      "  [ 77]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 78]\n",
      "  [ 83]\n",
      "  [ 83]\n",
      "  [ 66]\n",
      "  [111]\n",
      "  [123]\n",
      "  [ 78]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 85]\n",
      "  [134]\n",
      "  [ 74]\n",
      "  [ 85]\n",
      "  [ 69]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 81]\n",
      "  [ 75]\n",
      "  [ 61]\n",
      "  [151]\n",
      "  [115]\n",
      "  [ 91]\n",
      "  [ 12]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 10]\n",
      "  [ 85]\n",
      "  [153]\n",
      "  [ 83]\n",
      "  [ 80]\n",
      "  [ 68]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 80]\n",
      "  [ 68]\n",
      "  [ 61]\n",
      "  [162]\n",
      "  [122]\n",
      "  [ 78]\n",
      "  [  6]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 30]\n",
      "  [ 75]\n",
      "  [154]\n",
      "  [ 85]\n",
      "  [ 80]\n",
      "  [ 71]\n",
      "  [ 80]\n",
      "  [ 72]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 78]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 49]\n",
      "  [191]\n",
      "  [132]\n",
      "  [ 72]\n",
      "  [ 15]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 58]\n",
      "  [ 66]\n",
      "  [174]\n",
      "  [115]\n",
      "  [ 66]\n",
      "  [ 77]\n",
      "  [ 80]\n",
      "  [ 72]\n",
      "  [ 78]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 78]\n",
      "  [ 78]\n",
      "  [ 77]\n",
      "  [ 66]\n",
      "  [ 49]\n",
      "  [222]\n",
      "  [131]\n",
      "  [ 77]\n",
      "  [ 37]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 69]\n",
      "  [ 55]\n",
      "  [179]\n",
      "  [139]\n",
      "  [ 55]\n",
      "  [ 92]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 78]\n",
      "  [ 74]\n",
      "  [ 78]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 80]\n",
      "  [ 64]\n",
      "  [ 55]\n",
      "  [242]\n",
      "  [111]\n",
      "  [ 95]\n",
      "  [ 44]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 74]\n",
      "  [ 57]\n",
      "  [159]\n",
      "  [180]\n",
      "  [ 55]\n",
      "  [ 92]\n",
      "  [ 64]\n",
      "  [ 72]\n",
      "  [ 74]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 78]\n",
      "  [ 55]\n",
      "  [ 66]\n",
      "  [255]\n",
      "  [ 97]\n",
      "  [108]\n",
      "  [ 49]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 74]\n",
      "  [ 66]\n",
      "  [145]\n",
      "  [153]\n",
      "  [ 72]\n",
      "  [ 83]\n",
      "  [ 58]\n",
      "  [ 78]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 72]\n",
      "  [ 80]\n",
      "  [ 30]\n",
      "  [132]\n",
      "  [255]\n",
      "  [ 37]\n",
      "  [122]\n",
      "  [ 60]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 80]\n",
      "  [ 69]\n",
      "  [142]\n",
      "  [180]\n",
      "  [142]\n",
      "  [ 57]\n",
      "  [ 64]\n",
      "  [ 78]\n",
      "  [ 74]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 72]\n",
      "  [ 85]\n",
      "  [ 21]\n",
      "  [185]\n",
      "  [227]\n",
      "  [ 37]\n",
      "  [143]\n",
      "  [ 63]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 83]\n",
      "  [ 71]\n",
      "  [136]\n",
      "  [194]\n",
      "  [126]\n",
      "  [ 46]\n",
      "  [ 69]\n",
      "  [ 75]\n",
      "  [ 72]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 78]\n",
      "  [ 38]\n",
      "  [139]\n",
      "  [185]\n",
      "  [ 60]\n",
      "  [151]\n",
      "  [ 58]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  4]\n",
      "  [ 81]\n",
      "  [ 74]\n",
      "  [145]\n",
      "  [177]\n",
      "  [ 78]\n",
      "  [ 49]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 72]\n",
      "  [ 63]\n",
      "  [ 80]\n",
      "  [156]\n",
      "  [117]\n",
      "  [153]\n",
      "  [ 55]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 10]\n",
      "  [ 80]\n",
      "  [ 72]\n",
      "  [157]\n",
      "  [163]\n",
      "  [ 61]\n",
      "  [ 55]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 71]\n",
      "  [ 60]\n",
      "  [ 98]\n",
      "  [156]\n",
      "  [132]\n",
      "  [ 58]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 13]\n",
      "  [ 77]\n",
      "  [ 74]\n",
      "  [157]\n",
      "  [143]\n",
      "  [ 43]\n",
      "  [ 61]\n",
      "  [ 72]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 71]\n",
      "  [ 58]\n",
      "  [ 80]\n",
      "  [157]\n",
      "  [120]\n",
      "  [ 66]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 18]\n",
      "  [ 81]\n",
      "  [ 74]\n",
      "  [156]\n",
      "  [114]\n",
      "  [ 35]\n",
      "  [ 72]\n",
      "  [ 71]\n",
      "  [ 75]\n",
      "  [ 78]\n",
      "  [ 72]\n",
      "  [ 66]\n",
      "  [ 80]\n",
      "  [ 78]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 64]\n",
      "  [ 63]\n",
      "  [165]\n",
      "  [119]\n",
      "  [ 68]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 23]\n",
      "  [ 85]\n",
      "  [ 81]\n",
      "  [177]\n",
      "  [ 57]\n",
      "  [ 52]\n",
      "  [ 77]\n",
      "  [ 71]\n",
      "  [ 78]\n",
      "  [ 80]\n",
      "  [ 72]\n",
      "  [ 75]\n",
      "  [ 74]\n",
      "  [ 77]\n",
      "  [ 77]\n",
      "  [ 75]\n",
      "  [ 64]\n",
      "  [ 37]\n",
      "  [173]\n",
      "  [ 95]\n",
      "  [ 72]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 26]\n",
      "  [ 81]\n",
      "  [ 86]\n",
      "  [160]\n",
      "  [ 20]\n",
      "  [ 75]\n",
      "  [ 77]\n",
      "  [ 77]\n",
      "  [ 80]\n",
      "  [ 78]\n",
      "  [ 80]\n",
      "  [ 89]\n",
      "  [ 78]\n",
      "  [ 81]\n",
      "  [ 83]\n",
      "  [ 80]\n",
      "  [ 74]\n",
      "  [ 20]\n",
      "  [177]\n",
      "  [ 77]\n",
      "  [ 74]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 49]\n",
      "  [ 77]\n",
      "  [ 91]\n",
      "  [200]\n",
      "  [  0]\n",
      "  [ 83]\n",
      "  [ 95]\n",
      "  [ 86]\n",
      "  [ 88]\n",
      "  [ 88]\n",
      "  [ 89]\n",
      "  [ 88]\n",
      "  [ 89]\n",
      "  [ 88]\n",
      "  [ 83]\n",
      "  [ 89]\n",
      "  [ 86]\n",
      "  [  0]\n",
      "  [191]\n",
      "  [ 78]\n",
      "  [ 80]\n",
      "  [ 24]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 54]\n",
      "  [ 71]\n",
      "  [108]\n",
      "  [165]\n",
      "  [  0]\n",
      "  [ 24]\n",
      "  [ 57]\n",
      "  [ 52]\n",
      "  [ 57]\n",
      "  [ 60]\n",
      "  [ 60]\n",
      "  [ 60]\n",
      "  [ 63]\n",
      "  [ 63]\n",
      "  [ 77]\n",
      "  [ 89]\n",
      "  [ 52]\n",
      "  [  0]\n",
      "  [211]\n",
      "  [ 97]\n",
      "  [ 77]\n",
      "  [ 61]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 68]\n",
      "  [ 91]\n",
      "  [117]\n",
      "  [137]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 18]\n",
      "  [216]\n",
      "  [ 94]\n",
      "  [ 97]\n",
      "  [ 57]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 54]\n",
      "  [115]\n",
      "  [105]\n",
      "  [185]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  1]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [153]\n",
      "  [ 78]\n",
      "  [106]\n",
      "  [ 37]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [ 18]\n",
      "  [ 61]\n",
      "  [ 41]\n",
      "  [103]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [106]\n",
      "  [ 47]\n",
      "  [ 69]\n",
      "  [ 23]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  [  0]]], shape=(28, 28, 1), dtype=uint8)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 14:20:56.300906: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for item in mnist_train.take(1):\n",
    "    print(type(item))\n",
    "    print(item.keys())\n",
    "    print(item['image'])\n",
    "    print(item['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d3195b",
   "metadata": {},
   "source": [
    "이미지 항목은 tf.Tensor 타입으로 크기가 28x28인 배열이다.\n",
    "\n",
    "픽셀 강도를 나타내는 0-255 사이의 값이 들어있다.\n",
    "\n",
    "레이블은 tf.Tensor(2, shape=(), dtype=int64)로 출력되며 해당 이미지가 클래스 2에 해당하는 것을 확인할 수 있다.\n",
    "        \n",
    "        \n",
    "\n",
    "데이터셋을 로드할 때 with_info 매개변수를 사용하면 데이터셋에 대한 정보 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fc04018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='fashion_mnist',\n",
      "    full_name='fashion_mnist/3.0.1',\n",
      "    description=\"\"\"\n",
      "    Fashion-MNIST is a dataset of Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n",
      "    \"\"\",\n",
      "    homepage='https://github.com/zalandoresearch/fashion-mnist',\n",
      "    data_dir='/Users/chanhyeongkim/tensorflow_datasets/fashion_mnist/3.0.1',\n",
      "    file_format=tfrecord,\n",
      "    download_size=29.45 MiB,\n",
      "    dataset_size=36.42 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@article{DBLP:journals/corr/abs-1708-07747,\n",
      "      author    = {Han Xiao and\n",
      "                   Kashif Rasul and\n",
      "                   Roland Vollgraf},\n",
      "      title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\n",
      "                   Algorithms},\n",
      "      journal   = {CoRR},\n",
      "      volume    = {abs/1708.07747},\n",
      "      year      = {2017},\n",
      "      url       = {http://arxiv.org/abs/1708.07747},\n",
      "      archivePrefix = {arXiv},\n",
      "      eprint    = {1708.07747},\n",
      "      timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},\n",
      "      biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-07747},\n",
      "      bibsource = {dblp computer science bibliography, https://dblp.org}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mnist_test, info = tfds.load(name='fashion_mnist', with_info='true')\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4f421",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f47a2da8",
   "metadata": {},
   "source": [
    "### 케라스 모델에서 텐서플로 데이터셋 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1cfdd1",
   "metadata": {},
   "source": [
    "2장에선 간단하게 패션 MNIST를 사용하는 코드는 다음과 같았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bcf7234",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_trian), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef932f6",
   "metadata": {},
   "source": [
    "텐서플로 데이터셋을 사용하는 방법도 매우 비슷하지만 조금 차이가 있다.\n",
    "\n",
    "케라스 데이터셋은 ndarray 타입이다. 하지만 ***텐서플로 데이터셋은 Dataset 객체나 Tensor 타입을 반환*** 하기 떄문에 넘파이 배열로 바꾸려면 변환 작업이 약간 추가 된다\n",
    "\n",
    "**** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a7ecde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tfds.as_numpy(tfds.load('fashion_mnist',\n",
    "                                                    split=['train', 'test'],\n",
    "                                                    batch_size=-1,\n",
    "                                                    as_supervised=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae7149c",
   "metadata": {},
   "source": [
    "1. tfds.load에 원하는 데이터셋 이름 지정\n",
    "2. 이 데이터셋은 훈련,테스트 세트로 나눠져 있다는 걸 알고 있으므로 split 매개변수에 가져올 분할 세트 지정 가능\n",
    "3. batch_size=-1로 지정하면 모든 데이터를 가져온다.\n",
    "4. as_supervised=True로 지정하면 (입력, 레이블)로 구성된 튜플 반환\n",
    "***\n",
    "텐서플로 데이터셋에서 반환된 데이터셋은 케라스 데이터셋과 거의 포맷이 동일하다. \n",
    "\n",
    "한 가지 다른 점은 텐서플로 데이터셋은 (28, 28, 1) 크기를 반환하고 케라스 데이터셋은 (28, 28)크기 반환.\n",
    "\n",
    "따라서 입력크기를 (28, 28, 1)로 지정해줘야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5af60737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 527us/step - loss: 0.5292 - accuracy: 0.8125\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 530us/step - loss: 0.4003 - accuracy: 0.8542\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 523us/step - loss: 0.3684 - accuracy: 0.8651\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 530us/step - loss: 0.3450 - accuracy: 0.8725\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 528us/step - loss: 0.3335 - accuracy: 0.8768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2c2885d50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tfds.load('fashion_mnist',\n",
    "                                                split=['train', 'test'],\n",
    "                                                batch_size=-1,\n",
    "                                                as_supervised=True)\n",
    "X_train = tf.cast(X_train, tf.float32) / 255.0 # cast로 데이터타입 변경\n",
    "X_test = tf.cast(X_test, tf.float32) / 255.0\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5be81f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmax(model.predict(X_train[:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b138258e",
   "metadata": {},
   "source": [
    "조금 더 복잡한 예제는 3장에서 다뤘던 말-사람 데이터셋이다. 이 데이터셋도 텐서플로 데이터셋에서 제공한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "734679fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 1.1915 - acc: 0.8773\n",
      "Epoch 2/10\n",
      "103/103 [==============================] - 10s 100ms/step - loss: 0.0902 - acc: 0.9718\n",
      "Epoch 3/10\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.2223 - acc: 0.9270\n",
      "Epoch 4/10\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.0372 - acc: 0.9903\n",
      "Epoch 5/10\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 3.7826e-04 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 6.8735e-05 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "103/103 [==============================] - 11s 104ms/step - loss: 4.5185e-05 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "103/103 [==============================] - 11s 104ms/step - loss: 3.1394e-05 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "103/103 [==============================] - 11s 104ms/step - loss: 2.1619e-05 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2c2715890>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "data = tfds.load('horses_or_humans', split='train', as_supervised=True)\n",
    "\n",
    "train_batches = data.shuffle(100).batch(10)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(train_batches, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce72270",
   "metadata": {},
   "source": [
    "위에서 볼 수 있듯이 사용법이 아주 쉽다. 간단히 원하는 분할세트(여기에서는 train)을 지정해 load를 호출하고 모델 사용.\n",
    "\n",
    "효율적인 훈련을 위해서 데이터를 섞고 배치(세트)를 구성했다.\n",
    "\n",
    "말사람 데이터셋은 훈련-테스트로 나뉘어져 있으므로 훈련하는 동안 모델 검증을 원한다면 \n",
    "\n",
    "텐서플로 데이터셋에서 별도의 검증 세트 로드 가능(밑에 코드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2239bccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 1.4865e-05 - acc: 1.0000 - val_loss: 4.5882 - val_acc: 0.8555\n",
      "Epoch 2/10\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 1.0292e-05 - acc: 1.0000 - val_loss: 4.5424 - val_acc: 0.8633\n",
      "Epoch 3/10\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 8.3513e-06 - acc: 1.0000 - val_loss: 4.5498 - val_acc: 0.8672\n",
      "Epoch 4/10\n",
      "103/103 [==============================] - 11s 112ms/step - loss: 6.1173e-06 - acc: 1.0000 - val_loss: 4.6101 - val_acc: 0.8672\n",
      "Epoch 5/10\n",
      "103/103 [==============================] - 11s 112ms/step - loss: 4.9191e-06 - acc: 1.0000 - val_loss: 4.6222 - val_acc: 0.8633\n",
      "Epoch 6/10\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 3.9941e-06 - acc: 1.0000 - val_loss: 4.6553 - val_acc: 0.8633\n",
      "Epoch 7/10\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 3.2657e-06 - acc: 1.0000 - val_loss: 4.6786 - val_acc: 0.8633\n",
      "Epoch 8/10\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 2.9366e-06 - acc: 1.0000 - val_loss: 4.6728 - val_acc: 0.8633\n",
      "Epoch 9/10\n",
      "103/103 [==============================] - 11s 111ms/step - loss: 2.5411e-06 - acc: 1.0000 - val_loss: 4.7481 - val_acc: 0.8633\n",
      "Epoch 10/10\n",
      "103/103 [==============================] - 12s 114ms/step - loss: 2.1842e-06 - acc: 1.0000 - val_loss: 4.7486 - val_acc: 0.8633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2c2bd1690>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = tfds.load('horses_or_humans', split='test', as_supervised='true')\n",
    "\n",
    "validation_batches = val_data.batch(32)\n",
    "\n",
    "model.fit(train_batches, epochs=10, validation_data=validation_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b77d2e0",
   "metadata": {},
   "source": [
    "#### 특정 버전의 데이터셋 로드\n",
    "\n",
    "텐서플로 데이터셋에 있는 모든 데이터셋은 **MAJOR.MINOR.PATCH** 버전 번호 시스텝을 사용한다\n",
    "\n",
    "이 시스템은 다음처럼 관리된다.\n",
    "- PATCH가 업데이트 되면 동일한 데이터가 반환되지만 내부 구성이 바뀌었을 수 있다. 이런 변화는 개발자에게 알려지지 않음\n",
    "- MINOR가 업데이트 되면 기존 코드가 깨지지 않도록 데이터는 여전히 그대로지만 레코드마다 특성이 추가되었을 수 있다.\n",
    "- 또한 특정 슬라이스의 데이터가 동일해야 하므로 레코드 순서가 바끼지 않는다. MINOR가 업데이트 되면 레코드 형태와 위치에 변화가 있을 수 있다. 따라서 특정 슬라이스는 다른 값을 반환할 수 있다.\n",
    "\n",
    "다음처럼 버전을 명시적으로 지정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58449b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, info = tfds.load('horses_or_humans:3.0.0', with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e52c5",
   "metadata": {},
   "source": [
    "NonMathingCheckssumError 에러가 발생하면 수동으로 다운로드 하거나 tesorflow.org/datasets/overviiew#manual_download_if_download_fails 페이지 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fbe436",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9332e976",
   "metadata": {},
   "source": [
    "### 데이터 증식을 위해 매핑 함수 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cdc6f3",
   "metadata": {},
   "source": [
    "3장에서 훈련 데이터를 제공할 때 유용한 방법인 ImageDataGenerator를 사용한 이미지 증식 기법을 알아보았다.\n",
    "\n",
    "이전처럼 디렉터리에서 이미지를 읽지 않기 떄문에 텐서플로 데이터셋을 사용할 때 이런 이미지 증식을 어떻게 수행하는지 궁금할 수 있다.\n",
    "\n",
    "가장 좋은 방법은 데이터셋 객체에 매핑 함수를 사용하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a636ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tfds.load('horses_or_humans', split='train', as_supervised=True)\n",
    "train_batches = data.shuffle(100).batch(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02de4596",
   "metadata": {},
   "source": [
    "데이터셋을 변환해 매핑하려면 ***매핑함수**를 만들어야 한다. 이 함수는 평볌한 파이썬 함수이다.\n",
    "\n",
    "예를 들어 augmentimages 함수를 만들어 다음과 같이 이미지 증식을 수행하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "059e859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentimages(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image/255)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca400d",
   "metadata": {},
   "source": [
    "이 함수를 다음과 같이 데이터에 매핑해 새로운 데이터셋 train을 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12fc0b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.map(augmentimages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a9190",
   "metadata": {},
   "source": [
    "그다음 data가 아니라 train에서 배치를 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0972759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = train.shuffle(100).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5612ef1",
   "metadata": {},
   "source": [
    "tf.image 라이브러리에는 이미지 증식에 사용할 수 있는 함수가 많다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b8c20",
   "metadata": {},
   "source": [
    "#### 텐서플로 애드온 사용하기\n",
    "텐서플로 에드온 라이브러리는 더 많은 함수를 제공한다. ImageDataGenerator에 있는 일부 기능(ex.rotate)은 텐서플로 에드온에서만 제공하기 떄문에 알아두는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfd5ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff57e6",
   "metadata": {},
   "source": [
    "라이브러리 설치하면 매핑 함수에서 에드온 기능을 사용할 수 있다. 다음은 앞에서 만든 매핑 함수에 rotate 추가한 예이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c39f579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "def augmentimages(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image/255)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tfa.image.rotate(image, 40, interpolation='NEAREST')\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ee9c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50f4269d",
   "metadata": {},
   "source": [
    "### 사용자 정의 분할 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5928b",
   "metadata": {},
   "source": [
    "처음 1만개 데이터 불러옴(슬라이싱으로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82147c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tfds.load('cats_vs_dogs', split='train[:10000]', as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d956262",
   "metadata": {},
   "source": [
    "%를 사용해 데이터 불러오기(처음부터 20% 분량)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21f1379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tfds.load('cats_vs_dogs', split='train[:20%]', as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f3d85",
   "metadata": {},
   "source": [
    "독특하게 두 개의 분할을 합칠 수도 있다. 마지막 1000개와 처음 1000개의 레코드를 합쳐서 훈련 데이터로 사용하고 싶다면 다음처럼 쓸 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f53f5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tfds.load('cats_vs_dogs', split='train[-1000:] + train[:1000]', as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ceba5",
   "metadata": {},
   "source": [
    "강아지-고양이 데이터셋은 훈련세트, 테스트세트, 검증세트 분할이 없지만 텐서플로 데이터셋을 사용해 간단히 세트를 분할할 수 있다.\n",
    "\n",
    "데이터셋을 80%, 10%, 10% 비율로 나눈다 가정하면 다음과 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1560814",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tfds.load('cats_vs_dogs', split='train[:80%]', as_supervised=True)\n",
    "\n",
    "val_data = tfds.load('cats_vs_dogs', split='train[80:90%]', as_supervised=True)\n",
    "\n",
    "test_data = tfds.load('cats_vs_dogs', split='train[-10%:]', as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ab575",
   "metadata": {},
   "source": [
    "이렇게 얻은 세트는 기존과 동일한 방식으로 사용 가능\n",
    "\n",
    "반환된 데이터셋은 길이 정보를 제공하지 않기 때문에 원본 세트를 올바르게 나누었는지 확인하는 것이 어렵다.\n",
    "\n",
    "얼마나 많은 레코드가 분할세트에 포함되었는지 확인하려면 전체 세트를 순회하며 하나씩 카운트해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e088376f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18610\n"
     ]
    }
   ],
   "source": [
    "train_length = [i for i, _ in enumerate(train_data)][-1] + 1\n",
    "print(train_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa476c88",
   "metadata": {},
   "source": [
    "tf.data.experimental.cardinality 함수를 사용해 카운트 가능\n",
    "\n",
    "tf.int64 타입의 텐서를 반환하므로 다음처럼 numpy() 메서드로 넘파이 스칼라 값으로 바꾸어 출력 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "377920ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18610"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.data.experimental.cardinality(train_data).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b1c61",
   "metadata": {},
   "source": [
    "fashion_mnist 처럼 미리 분할된 데이터셋은 with_info=True로 지정해 분할세트 크기 확인 가능(위에 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7064da3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6c106c4",
   "metadata": {},
   "source": [
    "### TFRecord 이해하기\n",
    "텐서플로 데이터셋은 데이터를 다운로드해 디스크에 캐싱한다. 따라서 사용할 때마다 데이터를 다운로드하지 않는다.\n",
    "\n",
    "텐서플로 데이터셋은 캐싱을 위해 TFRecord 포맷을 사용한다. 이는 텐서플로에서 대용량의 데이터를 저장하고 추출할 때 선호하는 포맷이다.\n",
    "\n",
    "파일 구조는 매우 단순하며 성능을 위해 순차적으로 읽는다.\n",
    "\n",
    "디스크에 저장된 파일은 매우 간단하며 각 레코드는 레코드의 길이를 나타내는 정수, 이 정수의 CRC(순환 중복 검사) 데이터의 바이트 배열, 바이트 배열의 CRC로 구성.\n",
    "\n",
    "레코드가 연결되어 파일을 구성하고 대규모 데이터셋일 경우 샤딩(sharding) 된다.\n",
    "- 샤딩은 각 DB 서버에서 데이터를 분할하여 저장하는 방식이다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334181ea",
   "metadata": {},
   "source": [
    "MNIST 데이터셋을 다운로드하고 정보를 출력해 간단한 예를 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a291ceb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='mnist',\n",
      "    full_name='mnist/3.0.1',\n",
      "    description=\"\"\"\n",
      "    The MNIST database of handwritten digits.\n",
      "    \"\"\",\n",
      "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
      "    data_dir='/Users/chanhyeongkim/tensorflow_datasets/mnist/3.0.1',\n",
      "    file_format=tfrecord,\n",
      "    download_size=11.06 MiB,\n",
      "    dataset_size=21.00 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@article{lecun2010mnist,\n",
      "      title={MNIST handwritten digit database},\n",
      "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
      "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
      "      volume={2},\n",
      "      year={2010}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data, info = tfds.load('mnist', with_info=True)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cc22f5",
   "metadata": {},
   "source": [
    "features=FeaturesDict({\n",
    "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
    "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
    "    })\n",
    "    \n",
    "data_dir='/Users/chanhyeongkim/tensorflow_datasets/mnist/3.0.1.incomplete7GQAIK'\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10862ea7",
   "metadata": {},
   "source": [
    "다음처럼 원시 레코드를 TFRecord로 로드할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "292d62f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor: shape=(), dtype=string, numpy=b\"\\n\\x85\\x03\\n\\xf2\\x02\\n\\x05image\\x12\\xe8\\x02\\n\\xe5\\x02\\n\\xe2\\x02\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x00\\x00\\x00\\x00Wf\\x80H\\x00\\x00\\x01)IDAT(\\x91\\xc5\\xd2\\xbdK\\xc3P\\x14\\x05\\xf0S(v\\x13)\\x04,.\\x82\\xc5Aq\\xac\\xedb\\x1d\\xdc\\n.\\x12\\x87n\\x0e\\x82\\x93\\x7f@Q\\xb2\\x08\\xba\\tbQ0.\\xe2\\xe2\\xd4\\xb1\\xa2h\\x9c\\x82\\xba\\x8a(\\nq\\xf0\\x83Fh\\x95\\n6\\x88\\xe7R\\x87\\x88\\xf9\\xa8Y\\xf5\\x0e\\x8f\\xc7\\xfd\\xdd\\x0b\\x87\\xc7\\x03\\xfe\\xbeb\\x9d\\xadT\\x927Q\\xe3\\xe9\\x07:\\xab\\xbf\\xf4\\xf3\\xcf\\xf6\\x8a\\xd9\\x14\\xd29\\xea\\xb0\\x1eKH\\xde\\xab\\xea%\\xaba\\x1b=\\xa4P/\\xf5\\x02\\xd7\\\\\\x07\\x00\\xc4=,L\\xc0,>\\x01@2\\xf6\\x12\\xde\\x9c\\xde[t/\\xb3\\x0e\\x87\\xa2\\xe2\\xc2\\xe0A<\\xca\\xb26\\xd5(\\x1b\\xa9\\xd3\\xe8\\x0e\\xf5\\x86\\x17\\xceE\\xdarV\\xae\\xb7_\\xf3AR\\r!I\\xf7(\\x06m\\xaaE\\xbb\\xb6\\xac\\r*\\x9b$e<\\xb8\\xd7\\xa2\\x0e\\x00\\xd0l\\x92\\xb2\\xd5\\x15\\xcc\\xae'\\x00\\xf4m\\x08O'+\\xc2y\\x9f\\x8d\\xc9\\x15\\x80\\xfe\\x99[q\\x962@CN|i\\xf7\\xa9!=\\xd7 \\xab\\x19\\x00\\xc8\\xd6\\xb8\\xeb\\xa1\\xf0\\xd8l\\xca\\xfb]\\xee\\xfb]*\\x9fV\\xe1\\x07\\xb7\\xc9\\x8b55\\xe7M\\xef\\xb0\\x04\\xc0\\xfd&\\x89\\x01<\\xbe\\xf9\\x03*\\x8a\\xf5\\x81\\x7f\\xaa/2y\\x87ks\\xec\\x1e\\xc1\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82\\n\\x0e\\n\\x05label\\x12\\x05\\x1a\\x03\\n\\x01\\x02\">\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_name = os.path.join(os.path.expanduser('~') +\n",
    "                         '/tensorflow_datasets/mnist/3.0.1/mnist-test.tfrecord-00000-of-00001')\n",
    "raw_dataset = tf.data.TFRecordDataset(file_name)\n",
    "\n",
    "for raw_record in raw_dataset.take(1): # 원시레코드 내용 출력\n",
    "    print(repr(raw_record))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b7afbb",
   "metadata": {},
   "source": [
    "**체크섬** 과 레코드 내용이 포함된 긴 문자열 출력\n",
    "\n",
    "이 데이터셋의 특성을 이미 알고 있기 때문에 **특성 디스크립션** 을 만들어 데이터를 파싱할 수 있다.\n",
    "\n",
    "데이터를 파싱하는 코드는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d73f05c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <tf.Tensor: shape=(), dtype=string, numpy=b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x00\\x00\\x00\\x00Wf\\x80H\\x00\\x00\\x01)IDAT(\\x91\\xc5\\xd2\\xbdK\\xc3P\\x14\\x05\\xf0S(v\\x13)\\x04,.\\x82\\xc5Aq\\xac\\xedb\\x1d\\xdc\\n.\\x12\\x87n\\x0e\\x82\\x93\\x7f@Q\\xb2\\x08\\xba\\tbQ0.\\xe2\\xe2\\xd4\\xb1\\xa2h\\x9c\\x82\\xba\\x8a(\\nq\\xf0\\x83Fh\\x95\\n6\\x88\\xe7R\\x87\\x88\\xf9\\xa8Y\\xf5\\x0e\\x8f\\xc7\\xfd\\xdd\\x0b\\x87\\xc7\\x03\\xfe\\xbeb\\x9d\\xadT\\x927Q\\xe3\\xe9\\x07:\\xab\\xbf\\xf4\\xf3\\xcf\\xf6\\x8a\\xd9\\x14\\xd29\\xea\\xb0\\x1eKH\\xde\\xab\\xea%\\xaba\\x1b=\\xa4P/\\xf5\\x02\\xd7\\\\\\x07\\x00\\xc4=,L\\xc0,>\\x01@2\\xf6\\x12\\xde\\x9c\\xde[t/\\xb3\\x0e\\x87\\xa2\\xe2\\xc2\\xe0A<\\xca\\xb26\\xd5(\\x1b\\xa9\\xd3\\xe8\\x0e\\xf5\\x86\\x17\\xceE\\xdarV\\xae\\xb7_\\xf3AR\\r!I\\xf7(\\x06m\\xaaE\\xbb\\xb6\\xac\\r*\\x9b$e<\\xb8\\xd7\\xa2\\x0e\\x00\\xd0l\\x92\\xb2\\xd5\\x15\\xcc\\xae'\\x00\\xf4m\\x08O'+\\xc2y\\x9f\\x8d\\xc9\\x15\\x80\\xfe\\x99[q\\x962@CN|i\\xf7\\xa9!=\\xd7 \\xab\\x19\\x00\\xc8\\xd6\\xb8\\xeb\\xa1\\xf0\\xd8l\\xca\\xfb]\\xee\\xfb]*\\x9fV\\xe1\\x07\\xb7\\xc9\\x8b55\\xe7M\\xef\\xb0\\x04\\xc0\\xfd&\\x89\\x01<\\xbe\\xf9\\x03*\\x8a\\xf5\\x81\\x7f\\xaa/2y\\x87ks\\xec\\x1e\\xc1\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82\">, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=2>}\n"
     ]
    }
   ],
   "source": [
    "# 특성 디스크립션을 만든다\n",
    "feature_description = {\n",
    "    'image': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], dtype=tf.int64)\n",
    "}\n",
    "\n",
    "def _parse_func(example_proto):\n",
    "    # 위에서 만든 딕셔너리로 입력을 파싱한다.\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "parsed_dataset = raw_dataset.map(_parse_func)\n",
    "for parsed_record in parsed_dataset.take(1):\n",
    "    print((parsed_record))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435bb334",
   "metadata": {},
   "source": [
    "##### 참고\n",
    "parsing은 구문 분석이라고 합니다. 문장이 이루고 있는 구성 성분을 분해하고 분해된 성분의 위계 관계를 분석하여 구조를 결정하는 것입니다. \n",
    "\n",
    "즉 데이터를 분해 분석하여 원하는 형태로 조립하고 다시 빼내는 프로그램을 말합니다. 웹상에서 주어진 정보를 내가 원하는 형태로 가공하여 서버에서 불러들이는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a640372",
   "metadata": {},
   "source": [
    "파싱된 결과는 좀 더 알아보기 쉽다. 무엇보다도 이미지가 Tensor이고 PNG 형식임을 확인할 수 있다.(numpy=b\"\\x89PNG\\.. , 첫줄)\n",
    "\n",
    "PNG는 압축 이미지 포맷으로 헤더는 IHDR로 시작하고 데이터는 IDAT로 시작하며 IEND로 끝난다.(파싱된 결과 위에 보면 있음)\n",
    "\n",
    "이제 원시 TFReocord 파일을 읽어서 Pillow 같은 라이브러리를 사용해 PNG 이미지로 디코딩 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a83c3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d89418e",
   "metadata": {},
   "source": [
    "### 텐서플로에서 데이터 관리를 위한 ETL 프로세스\n",
    "ETL은 규모와 상관없이 텐서플로의 훈련에 사용되는 핵심 패턴이다. \n",
    "\n",
    "***\n",
    "ETL 프로세스의 \n",
    "- **추출단계**는 다른 곳에 저장되어 있는 원시 데이터를 가져와 변환을 위해 준비하는 단계이다.\n",
    "\n",
    "- **변환단계** 는 훈련에 적합하도록 또는 훈련이 잘되도록 데이터를 조작하는 단계이다. 예를 들어 이미지증식, 특성열매핑, 또는 데이터에 적용하는 다른 변환 로직 등.\n",
    "\n",
    "- **로드(적재)단계** 는 훈련을 위해 신경망에 데이터를 로드하는 단계이다.\n",
    "\n",
    "다음은 말-사람 데이터셋으로 분류기를 훈련하는 전체코드이다. 추출, 변환, 로드 단계를 구분할 수 있도록 주석을 추가함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f31e67",
   "metadata": {},
   "source": [
    "#### 말-사람 데이터 훈련 전체 소스코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de5e499b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "33/33 [==============================] - 11s 318ms/step - loss: 0.4940 - acc: 0.7527 - val_loss: 255.7508 - val_acc: 0.5977\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - 10s 312ms/step - loss: 0.1326 - acc: 0.9494 - val_loss: 290.4411 - val_acc: 0.5391\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - 10s 311ms/step - loss: 0.0999 - acc: 0.9630 - val_loss: 244.2785 - val_acc: 0.5469\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - 10s 311ms/step - loss: 0.0272 - acc: 0.9912 - val_loss: 336.0301 - val_acc: 0.4648\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - 10s 313ms/step - loss: 0.0318 - acc: 0.9893 - val_loss: 370.6453 - val_acc: 0.4570\n",
      "Epoch 6/10\n",
      "33/33 [==============================] - 10s 312ms/step - loss: 0.1828 - acc: 0.9367 - val_loss: 321.5753 - val_acc: 0.5664\n",
      "Epoch 7/10\n",
      "33/33 [==============================] - 10s 309ms/step - loss: 0.0649 - acc: 0.9796 - val_loss: 361.9042 - val_acc: 0.4258\n",
      "Epoch 8/10\n",
      "33/33 [==============================] - 10s 307ms/step - loss: 0.0302 - acc: 0.9903 - val_loss: 450.4013 - val_acc: 0.4375\n",
      "Epoch 9/10\n",
      "33/33 [==============================] - 10s 308ms/step - loss: 0.0151 - acc: 0.9961 - val_loss: 492.3778 - val_acc: 0.4336\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 10s 310ms/step - loss: 0.0077 - acc: 0.9961 - val_loss: 676.6941 - val_acc: 0.4180\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# 모델 정의 #\n",
    "model2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# 추출 단계 #\n",
    "data = tfds.load('horses_or_humans', split='train', as_supervised=True)\n",
    "\n",
    "val_data = tfds.load('horses_or_humans', split='test', as_supervised=True)\n",
    "\n",
    "\n",
    "# 변환 단계 #\n",
    "\n",
    "def augmentimages(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image/255)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tfa.image.rotate(image, 40, interpolation='NEAREST')\n",
    "    return image, label\n",
    "\n",
    "train = data.map(augmentimages)\n",
    "train_batches = train.shuffle(100).batch(32)\n",
    "val_batches = val_data.batch(32)\n",
    "\n",
    "# 로드 단계 #\n",
    "history = model2.fit(train_batches, epochs=10, validation_data=val_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214924ae",
   "metadata": {},
   "source": [
    "이런 프로세스를 사용하며 데이터 파이프라인이 데이터나 모델 구조의 변화에 덜 민감하도록 만들 수 있다.\n",
    "\n",
    "- ***텐서플로 데이터셋을 이용해 데이터를 추출하는 구조는 데이터의 크기와는 상관없이 동일하다***\n",
    "\n",
    "- 변환을 위한 tf.data API도 일관성 있어 데이터 소스와는 상관없이 비슷한 변환 적용 가능\n",
    "\n",
    "- 물론 변환이 되고 난 후 데이터 로딩 단계도 하나의 cpu나 하나의 gpu, gpu 클러스터 또는 tpu 포드에서 훈련하는지와는 상관없이 동일하다. 하지만 데이터를 로드하는 단계가 훈련 속도에 큰 영향을 미칠 수는 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354a81c",
   "metadata": {},
   "source": [
    "#### 로드 단계 최적화 하기\n",
    "\n",
    "CPU가 작업을 준비하는 동안 GPU/TPU는 유휴상태이다(아무것도 안함).\n",
    "\n",
    "그리고 GPU/TPU에서 훈련을 하는데, 이떄는 CPU가 유휴상태이다.\n",
    "\n",
    "***\n",
    "\n",
    "이에 대해 데이터 준비와 훈련을 병렬로 수행하는 것이 논리적인 해결책이다.\n",
    "이런 과정을 **파이프라이닝**이라고 한다.\n",
    "- 첫번쨰 배치는 CPU가 준비하고, 두번쨰부터 GPU/TPU가 훈련하는 동안 CPU는 다음배치 준비를 한다.\n",
    "\n",
    "케라스의 패션 MNIST 같은 간단한 데이터셋에서 텐서프롤 데이터셋 버전으로 바꿀 떄 훈련 전에 배치를 만들어야 한다.\n",
    "\n",
    "이는 데이터셋이 얼마나 큰지와는 상관없이 파이프라이닝을 사용하기 떄문이다. 따라서 일관된 ETL 패턴을 계속 사용할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2780b0",
   "metadata": {},
   "source": [
    "#### 훈련속도 향상을 위해 ETL 병렬화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa36a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, info = tfds.load('cats_vs_dogs', split='train', with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5048913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='cats_vs_dogs',\n",
      "    full_name='cats_vs_dogs/4.0.1',\n",
      "    description=\"\"\"\n",
      "    A large set of images of cats and dogs. There are 1738 corrupted images that are dropped.\n",
      "    \"\"\",\n",
      "    homepage='https://www.microsoft.com/en-us/download/details.aspx?id=54765',\n",
      "    data_dir='/Users/chanhyeongkim/tensorflow_datasets/cats_vs_dogs/4.0.1',\n",
      "    file_format=tfrecord,\n",
      "    download_size=786.67 MiB,\n",
      "    dataset_size=1.04 GiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(None, None, 3), dtype=uint8),\n",
      "        'image/filename': Text(shape=(), dtype=string),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'train': <SplitInfo num_examples=23262, num_shards=16>,\n",
      "    },\n",
      "    citation=\"\"\"@Inproceedings (Conference){asirra-a-captcha-that-exploits-interest-aligned-manual-image-categorization,\n",
      "    author = {Elson, Jeremy and Douceur, John (JD) and Howell, Jon and Saul, Jared},\n",
      "    title = {Asirra: A CAPTCHA that Exploits Interest-Aligned Manual Image Categorization},\n",
      "    booktitle = {Proceedings of 14th ACM Conference on Computer and Communications Security (CCS)},\n",
      "    year = {2007},\n",
      "    month = {October},\n",
      "    publisher = {Association for Computing Machinery, Inc.},\n",
      "    url = {https://www.microsoft.com/en-us/research/publication/asirra-a-captcha-that-exploits-interest-aligned-manual-image-categorization/},\n",
      "    edition = {Proceedings of 14th ACM Conference on Computer and Communications Security (CCS)},\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab66536",
   "metadata": {},
   "source": [
    "TFRecord를 사용하고 싶다면 다운로드된 원시 파일을 읽어야 한다. 데이터셋이 크다면 여러개의 파일에 나눠져있다.(버전4.0.0의 경우 파일 8개 )\n",
    "\n",
    "tf.data.Dataset.list_files를 사용해 로드할 파일 목록 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1b822f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = os.path.join(\n",
    "    os.path.expanduser('~') +\n",
    "    '/tensorflow_datasets/cats_vs_dogs/4.0.1/cats_vs_dogs-train.tfrecord*'\n",
    ")\n",
    "files = tf.data.Dataset.list_files(file_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37344efb",
   "metadata": {},
   "source": [
    "파일 목록을 얻으면 다음처럼 files.interleave를 사용해 데이터셋을 로드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0895134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = files.interleave(\n",
    "    tf.data.TFRecordDataset,\n",
    "    cycle_length=4,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf9dcc4",
   "metadata": {},
   "source": [
    "새롭게 등장한 개념이 몇 가지 있다.\n",
    "\n",
    "- **cycle_length** 매개변수는 동시에 전처리할 입력 원소 개수를 지정한다. 4로 지정했기 때문에 이 프로세스는 동시에 4개의 레코드를 처리한다. 이값을 지정하지 않으면 사용 가능한 CPU 코어 개수를 사용한다\n",
    "\n",
    "- **num_parallel_calls** 매개변수는 병렬 실행횟수를 지정한다. 여기서처럼 **tf.data.experimental.AUTOTUNE** 로 지정하면 CPU 개수에 따라 동적으로 설정되기 때문에 코드 이식성이 좋아진다.\n",
    "**cycle_length** 와 **num_parallel_calls** 를 같이 사용하면 병렬화를 최대로 활용 가능하다. \n",
    "\n",
    "예를 들어 자동으로 설정된 **num_parallel_calls** 값이 6이고, **cycle_length** 가 4이면 6개의 스레드가 각각 4개의 레코드를 동시에 로딩한다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e64f62",
   "metadata": {},
   "source": [
    "추출단계 병렬화에 이허 **변환단계 병렬화** 에 대해 알아보겠다. 먼저 원시 TFRecord 파일을 로드하고 적절한 형태로 바꾸는 매핑 함수를 만든다. \n",
    "\n",
    "예를 들어 JPEG 이미지를 디코딩해 이미지 버퍼에 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff1507b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(serialized_example):\n",
    "    feature_description={\n",
    "        'image':tf.io.FixedLenFeature((), tf.string, \"\"),\n",
    "        'label':tf.io.FixedLenFeature((), tf.int64, -1)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(\n",
    "        serialized_example, feature_description\n",
    "    )\n",
    "    \n",
    "    image = tf.io.decode_jpeg(example['image'], channels=3)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image / 255\n",
    "    image = tf.image.resize(image, (300, 300))\n",
    "    return image, example['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4632a1a",
   "metadata": {},
   "source": [
    "여기서 볼 수 있듯이 일반적인 매핑함수는 병렬 작업을 위해 특별히 처리하는 것이 없다.\n",
    "\n",
    "병렬화는 다음처럼 매핑함수를 호출할 때 수행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c07a67ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "print(cores)\n",
    "train_dataset = train_dataset.map(read_tfrecord, num_parallel_calls=cores)\n",
    "train_dataset = train_dataset.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed6d2fc",
   "metadata": {},
   "source": [
    "자동설정을 원치 않으면  multiprocessing 라이브러리를 사용해 CPU 개수를 카운트해준다. 그다음 매핑 함수를 호출할 때 병렬실행횟수에 이 값을 전달한다. 간단하다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7aa9ec",
   "metadata": {},
   "source": [
    "cache 메서드는 메모리에 데이터셋을 캐싱한다.\n",
    "\n",
    "램이 충분하다면 속도 향상에 아주 좋다. 강아지-고양이 데이터셋으로 콜랩에서 실행하면 램이 부족해서 오류 발생.\n",
    "\n",
    "로딩과 훈련도 병렬화 할 수 있다. 데이터를 섞고 배치를 만드는 것은 물론 다음 코드처럼 사용 가능한 CPU 코어 개수를 바탕으로 데이터를 **프리페치** 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9becc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(1024).batch(32)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a34e9",
   "metadata": {},
   "source": [
    "훈련세트 병렬화 준비를 마치면 이전처럼 모델을 훈련한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c17bca8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "    718/Unknown - 212s 295ms/step - loss: 0.6257 - acc: 0.6190"
     ]
    }
   ],
   "source": [
    "\n",
    "model3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model3.fit(train_dataset, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4232350c",
   "metadata": {},
   "source": [
    "왤케 오래 걸리냐?..\n",
    "\n",
    "맥으론 오래 걸려서 클라우드를 이용하는 코랩으로 돌려봄\n",
    "\n",
    "코랩에서도 안 된다. 용량이 너무 큰가 보다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82f207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
