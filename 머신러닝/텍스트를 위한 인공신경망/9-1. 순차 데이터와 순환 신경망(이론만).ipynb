{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca548a43",
   "metadata": {},
   "source": [
    "### 순차 데이터\n",
    "\n",
    "**순차 데이터 sequential data** 는 텍스트나 시계열 데이터 time seires data 와 **같이 순서에 의미가 있는 데이터를 말한다.** 예를 들어 \"I am a boy\"는 쉽게 이해 할수 있지만 \"boy am a I\"는 말이 되지 않는다. 또 일별 온도를 기록한 데이터에서 날짜 순서를 뒤죽박죽 섞는다면 내일의 온도르르 쉽게 예상하기 어렵다.\n",
    "\n",
    "지금까지 우리가 보았던 데이터는 순서와는 상관이 없다. 예로 패션 MNIST 데이터를 생각해보자. 이 데이터를 신경망 모델에 전달할 때 샘플을 랜덤하게 섞은 후 훈련 세트와 검증 세트로 나누었다. 즉 샘플의 순서와 전혀 상관이 없다. 심지어 골고루 섞는 편이 결과가 더 좋다.\n",
    "\n",
    "이는 딥러닝뿐만 아니라 일반적인 머신러닝 모델에서도 마찬가지이다. 4장에서 봤던 생선 데이터나 패션 MNIST 데이터는 어떤 샘플이 먼저 주입되어도 모델의 학습에 큰 영향을 미치지 않는다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acca8de7",
   "metadata": {},
   "source": [
    "이 장에서 사용하려는 댓글, 즉 텍스트 데이터는 단어의 순서가 중요한 순차 데이터이다. 이런 데이터는 순서를 유지하며 신경망에 주입해야 한다. 단어의 순서를 마구 섞어서 주입하면 안된다.\n",
    "\n",
    "따라서 순차 데이터를 다룰 때는 이전에 입력한 데이터를 기억하는 기능이 필요하다. 예를 들어 \"별로지만 추천해요\" 에서 \"추천해요\"가 입력될 때 \"별로지만\"을 기억하고 있어야 이 댓글을 무조건 긍정적이라고 판단하지 않을 것이다.\n",
    "\n",
    "***완전 연결 신경망이나 합성곱 신경망은 이런 기억 장치가 없다.*** 하나의 샘플(또는 하나의 배치)을 사용하여 정방향 계산을 수행하고 나면 그 샘플은 버려지고 다음 샘플을 처리할 때 ***재사용하지 않습니다.*** 이렇게 입력 데이터의 흐름이 앞으로만 전달되는 신경망을 **피드포워드 신경망 (feedforward neural network FFNN)이라고 한다.** 이전 장에서 배웠던 완전 연결 신경망과 합성곱 신경망이 모두 피드포워드 신경망에 속한다.\n",
    "\n",
    "신경망이 이전에 처리했던 샘플을 다음 샘플을 처리하는데 재사용하기 위해서는 이렇게 데이터 흐름이 앞으로만 전달되어서는 곤란하다. 다음 샘플을 위해서 이전 데이터가 신경망 층에 순환될 필요가 있다. 이런 신경망이 바로 **순환 신경망**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01bde8d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3ac5a8f",
   "metadata": {},
   "source": [
    "### 순환 신경망\n",
    "순환 신경망 recurrent neural network, RNN은 일반적인 완전 연결 신경망과 거의 비슷하다. 완전 연결 신경망에 이전 데이터의 처리 흐름을 순환하는 고리 하나만 추가하면 된다.\n",
    "\n",
    "그래서 순환 신경망에서는 '이전 샘플에 대한 기억을 가지고 있다'고 종종 말한다. 이렇게 샘플을 처리하는 한 단계를 **타임스텝 timestep**이라고 말한다. 순환 신경망은 이전 타임스텝의 샘플을 기억하지만 타임스텝이 오래 될수록 순환되는 정보는 희미해 진다. 나중에 여기에 대해서 다시 자세히 언급하겠다.    ***(p489-490에 그림으로 자세하게 설명)***\n",
    "\n",
    "순환 신경망에는 특별히 층을 셀 cell 이라고 부른다. 한 셀에는 여러 개의 뉴런이 있지만 완전 연결 신경망과 달리 뉴런을 모두 표시하지 않고 하나의 셀로 층을 표현한다. 또 셀의 출력을 **은닉 상태 hidden state**라고 부른다.\n",
    "\n",
    "합성곱 신경망에서처럼 신경망의 구조마다 조금씩 부르는 이름이 다를 수 있다. 하지만 기본 구조는 같다. 입력에 어떤 가중치를 곱하고 활성화 함수를 통과시켜 다음 층으로 보내는 거다. 달라지는 것은 층의 출력(즉 은닉 상태)을 다음 타임 스텝에 재사용한다는 것 뿐이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03277068",
   "metadata": {},
   "source": [
    "일반적으로 은닉층의 활성화 함수로는 하이퍼볼릭 탄젠트 hyperbolic tangent 함수인 tanh가 많이 사용된다. tanh함수도 S자 모양을 띠기 때문에 종종 시그모이드 함수라고 부르기도 한다. 하지만 헷갈릴 수 있으니 이 책에서는 이렇게 부르지 않겠다. tanh함수는 시그모이드 함수와는 달리 -1 ~ 1 사이의 범위를 가진다.\n",
    "\n",
    "다른 신경망과 마찬가지로 순환 신경망 그림에도 번거로움을 피하기 위해 활성화 함수를 표시하지 않는 경우가 많다. 하지만 순환 신경망에서도 활성화 함수가 반드시 필요하다는 것을 꼭 기억하자.\n",
    "\n",
    "합성곱 신경망과 같은 피드포워드 신경망에서 뉴런은 입력과 가중치를 곱한다. ***(여기서 곱해지는 가중치는 w_b로 동일하다)***\n",
    "\n",
    "순환 신경망에서도 동일하다. 다만 순환 신경망의 뉴런은 가중치가 하나 더 있다. 바로 이전 타임스텝의 은닉 상태에 곱해지는 가중치이다. 셀은 입력과 이전 타입스텝의 은닉 상태를 사용하여 현재 타임스텝의 은닉 상태를 만든다.\n",
    "\n",
    "- 맨 처음 타임스탭1에서 사용되는 이전 은닉 상태 h0는 어떻게 구할 수 있을까? \n",
    "    - 맨 처음 샘플을 입력할 때는 이전 타임스텝이 없다. 따라서 간단히 h0는 모두 0으로 초기화된다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff1af2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd43a124",
   "metadata": {},
   "source": [
    "### 셀의 가중치와 입출력\n",
    "\n",
    "순환 신경망의 셀에서 필요한 가중치 크기를 계산해 보겠다. 복잡한 모델을 배울수록 가중치 개수를 계산해 보면 잘 이해하고 알 수 있다. 예를 들어 다음 그림처럼 순환층에 입력되는 특성의 개수가 4개이고 순환층의 뉴런이 3개라고 가정해 보자. 먼저 Wx의 크기를 구해보자. **입력층과 순환층의 뉴런이 모두 완전 연결되기 때문에 가중치 Wx의 크기는 4 x 3 = 12개가 된다.** 7장에서 본 완전 연결 신경망의 입력층과 은닉층의 연결과 같다. 그럼 순환층에서 다음 타임스텝에 재사용되는 은닉 상태를 위한 가중치 Wb의 크기는 어떻게 될까?\n",
    "\n",
    "**(p.493 그림)**\n",
    "순환층에 있는 첫 번째 뉴런(r1)의 은닉 상태가 다음 타임스텝에 재사용될 때 첫 번째 뉴런과 두 번째, 세 번째 뉴런에 모두 전달 된다. 위 그림에서 붉은색으로 표시했다. 즉 이전 타임스텝의 은닉 상태는 다음 타임스텝의 뉴런에 완전히 연결된다.\n",
    "\n",
    "두 번째 뉴런의 은닉 상태도 마찬가지로 첫 번째 뉴런과 두 번째 뉴런, 세 번째 뉴런에 모두 전달되고(파란 화살표), 세 번째 뉴런의 은닉 상태도 동일하다(검은 화살표). ***따라서 이 순한층에서 은닉 상태를 위한 가중치 W3는 3 x 3 = 9 개이다.*** 가중치는 모두 구했으니 모델 파라미터 개수를 계산해 보자. 가중치에 절편을 더하면 된다. 여기엔 각 뉴런마다 하나의 절편이 있다. 따라서 이 순환층은 모두 ***12 + 9 + 3 =24개의 모델 파라미터를 가지고 있다.*** 이제 왜 순환층을 셀 하나로 표시할 수밖에 없는지 이해가 됬을 것이다. ***은닉 상태가 모든 뉴런에 순환되기 때문에 완전 연결 신경망처럼 그림으로 표현하기는 너무 어렵다.***\n",
    "\n",
    "- 모델 파라미터 수 = Wx + Wb + 절편 = 12 + 9 + 3 = 24\n",
    "\n",
    "순환층의 가중치 크기를 알아보았으므로 이번에는 순환층의 입력과 출력에 대해 생각해 보자. 이전 장에서 배웠던 합성곱 층의 입력은 전형적으로 하나의 샘플이 3개의 차원을 가진다. 너비, 높이, 채널이다. 입력이 합성곱 층과 풀링 층을 통과하면 너비,높이,채널(혹은 깊이)의 크기가 달라지지만 차원의 개수는 그대로 유지되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73c1a50",
   "metadata": {},
   "source": [
    "순환층의 가중치 크기를 알아보았으므로 이번에는 순환층의 입력과 출력에 대해 생각해 보자. 이전 장에서 배웠던 합성곱 층의 입력은 전형적으로 하나의 샘플이 3개의 차원을 가진다. 너비, 높이, 채널이다. 입력이 합성곱 층과 풀링 층을 통과하면 너비,높이,채널(혹은 깊이)의 크기가 달라지지만 차원의 개수는 그대로 유지되었다.\n",
    "\n",
    "\n",
    "**순환층은 일반적으로 샘플마다 2개의 차원을 가진다.** 보통 하나의 샘플을 하나의 시퀀스 sequence라고 말한다. 시퀀스 안에는 여러 개의 아이템이 들어 있다. **여기에서 시퀀스의 길이가 바로 타임스텝 길이가 된다.**\n",
    "\n",
    "\n",
    "##### (이 밑에 내용들은 p.493~ 그림과 자세히 나와있음)\n",
    "\n",
    "\n",
    "예를 들어 어떤 샘플에 \"I am a boy\" 란 문장이 들어 있다고 가정해 보자. 이 샘플은 4개의 단어로 이루어져 있다. 각 단어를 3개의 어떤 숫자로 표현한다고 가정해 보자.( 이 숫자 표현에 대해서는 다음 절에 자세히 다루겠다.)\n",
    "\n",
    "\n",
    "\n",
    "이런 입력이 순환층을 통과하면 두 번째, 세 번째 차원이 사라지고 순환층의 뉴런 개수만큼 출력된다. 이를 차근차근 설명해 보겠다. 하나의 샘플은 시퀀스 길이(여기서에서는 단어 개수) 와 단어표현(다음 절에서 다룬다)의 2차원 배열이다. 순환층을 통과하면 1차원 배열로 바뀐다. 이 1차원 배열의 크기는 순환층의 뉴런 개수에 의해 결정된다. 셀이 모든 타입스텝에서 출력을 만든 것처럼 표현했다. 하지만 ***사실 순환층은 기본적으로 마지막 타임스텝의 은닉 상태만 출력으로 내보낸다.***\n",
    "\n",
    "\n",
    "마지막으로 출력층의 구성에 대해 알아보겠다. 합성곱 신경망과 마찬가지로 순환 신경망도 마지막에는 밀집층을 두어 클래스를 분류한다. 다중 분류일 경우에는 출력층에 클래스 개수만큼 뉴런을 두고 소프트맥스 활성화 함수를 사용한다. 이진 분류일 경우에는 하나의 뉴런을 두고 시그모이드 활성화 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13de00b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
