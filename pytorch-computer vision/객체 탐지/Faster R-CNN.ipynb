{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chanhyeong00/machine_learning_study/blob/main/pytorch-computer%20vision/%EA%B0%9D%EC%B2%B4%20%ED%83%90%EC%A7%80/Faster%20R-CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MS COCO(Microsoft Common Objects in Context) 데이터 세트를 활용해 Faster R-CNN 모델을 미세 조정해 이미지를 분류해본다."
      ],
      "metadata": {
        "id": "1401jn38bHqq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZYktihbXIVE"
      },
      "outputs": [],
      "source": [
        "!unzip coco.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MS COCO 데이터세트는 경계 상자 탐지 및 객체 분할을 위한 정보를 제공한다. 모델 학습을 위한 **데이터 주석(Data Annotation)** 정보는 annotations 디렉터리에 제공"
      ],
      "metadata": {
        "id": "8PD_-yDIctZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 주석 파일은 JSON 형식으로 제공되며,\n",
        "\n",
        "정보, 라이선스, 카테고리, 이미지, 어노테이션 정보가 포함"
      ],
      "metadata": {
        "id": "8ykG6o7Yc41K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpzJ8wdBqHym",
        "outputId": "7ab6b7ec-d13f-4b73-8052-1920b36deed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cython pycocotools"
      ],
      "metadata": {
        "id": "t2D0j5s2clMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 세트"
      ],
      "metadata": {
        "id": "VzYaZbGido_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class COCODataset(Dataset):\n",
        "    # root: 데이터 세트 경로, train:학습 데이터 불러오기 여부(False면 검증용 데이터셋)\n",
        "    def __init__(self, root, train, transform=None):\n",
        "        super().__init__()\n",
        "        directory = \"train\" if train else \"val\"\n",
        "\n",
        "        # 디렉터리에 있는 어노테이션 JSON 파일 경로를 설정\n",
        "        annotations = os.path.join(root, \"annotations\", f\"{directory}_annotations.json\")\n",
        "\n",
        "        # 위 경로를 pycocotools.coco.COCO 클래스에 입력\n",
        "        self.coco = COCO(annotations)\n",
        "        self.iamge_path = os.path.join(root, directory)\n",
        "        self.transform = transform\n",
        "\n",
        "        # 어노테이션 정보 불러오기 전에 학습에 사용되는 카테고리 정보 불러오기\n",
        "        self.categories = self._get_categories() # 모델 추론 시 카테고리 정보를 확인하기 위함\n",
        "\n",
        "        # 이미지와 어노테이션 정보를 불러온다.\n",
        "        self.data = self._load_data()\n",
        "\n",
        "\n",
        "    def _get_categories(self):\n",
        "        categories = {0: \"background\"} # 0은 배경\n",
        "\n",
        "        for category in self.coco.cats.values(): # cats 속성에서 가테고리 정보 불러오기\n",
        "        # cats 속성은 딕셔너리로,\n",
        "        # 상위 카테고리(supercategory), 카테고리 ID(id), 카테고리 이름(name) 정보 포함\n",
        "            categories[category[\"id\"]] = category[\"name\"]\n",
        "        return categories\n",
        "\n",
        "    def _load_data(self):\n",
        "        data = []\n",
        "        # coco.imgs 속성은 어노테이션 JSON 파일의 이미지 정보(images)를 순차적으로 반환\n",
        "        # 어노테이션 정보는 이미지 ID와 매핑될 수 있으므로 이미지 ID(_id) 추출\n",
        "        for _id in self.coco.imgs:\n",
        "            # 입력된 이미지 ID를 받아 어노테이션 정보를 반환\n",
        "            # 한 번에 여러 ID를 입력받을 수 있어 리스트 형식을 반환\n",
        "            # 첫 번째 어노테이션 정보를 가져와 파일 이름 추출하고\n",
        "            file_name = self.coco.loadImgs(_id)[0][\"file_name\"]\n",
        "            # 이미지 경로 불러오기\n",
        "            image_path = os.path.join(self.iamge_path, file_name)\n",
        "            # 이미지 불러오기\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "            boxes = []\n",
        "            labels = []\n",
        "            # 어노테이션 ID를 coco.getAnnIds에 이미지 Id 입력하면 어노테이션 ID 반환\n",
        "            # 반환된 id를 coco.loadAnns로 전달해 어노테이션 정보 불러오기\n",
        "            anns = self.coco.loadAnns(self.coco.getAnnIds(_id))\n",
        "            for ann in anns: # 어노테이션 돌기\n",
        "                x, y, w, h = ann[\"bbox\"] # 경계 상자(bbox) 정보 추출\n",
        "\n",
        "                boxes.append([x, y, x + w, y + h]) # 경계 상자 정보 리스트에 저장\n",
        "                labels.append(ann[\"category_id\"]) # 레이블 저장\n",
        "\n",
        "            target = {\n",
        "            \"image_id\": torch.LongTensor([_id]),\n",
        "                \"boxes\": torch.FloatTensor(boxes), # 경계상자는 FloatTensor\n",
        "                \"labels\": torch.LongTensor(labels) # label은 LongTensor\n",
        "            }\n",
        "            data.append([image, target])\n",
        "        return data\n",
        "\n",
        "    # 호출 메서드\n",
        "    def __getitem__(self, index):\n",
        "        image, target = self.data[index]\n",
        "        if self.transform: # transform 변수가 있다면 이미지 변환을 적용한 이미지 반환\n",
        "            image = self.transform(image)\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "metadata": {
        "id": "f7VOu4b3df5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 로더"
      ],
      "metadata": {
        "id": "eY3hi1wVkEJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 데이터에 패딩 적용\n",
        "def collator(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.PILToTensor(), # PIL 이미지를 Tensor 로 변환\n",
        "        transforms.ConvertImageDtype(dtype=torch.float) # 텐서 이미지를 다시 float 형식으로 변환\n",
        "    ]\n",
        "    # Faster R-CNN이 float 형식의 0.0 ~ 1.0 범위를 갖는 이미지 텐서를 사용하기 때문이다.\n",
        "    # 또한, 서로 다른 크기의 이미지를 입력받아 모델 내부에서 크기를 변경하므로 크기를 조절하지 않아도 된다.\n",
        ")\n",
        "\n",
        "train_dataset = COCODataset(\"coco\", train=True, transform=transform)\n",
        "test_dataset = COCODataset(\"coco\", train=False, transform=transform)\n",
        "\n",
        "\n",
        "# COCO 데이터세트는 이미지 내에 여러 객체 정보가 담길 수 있으므로 데이터 길이가 다를 수 있다.\n",
        "# 그러므로 데이터 로더에 집합 함수(collator_fn)을 적용해 데이터를 패딩한다.\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=4, shuffle=True, drop_last=True, collate_fn=collator\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=1, shuffle=True, drop_last=True, collate_fn=collator\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSTaEomokEy1",
        "outputId": "c53cce62-c490-4278-fc08-5150086f6dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.07s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이미지의 크기를 조절하거나 대칭 등 어노테이션 정보가 달라지는 변환이 적용되면, 어노테이션 정보도 변경해야 한다.\n",
        "\n",
        "\n",
        "앞서 데이터 증가 예제처럼 클래스를 재정의하거나 파이토치 깃허브에서 제공하는 객체 검출을 위한 스키립트를 사용한다. 객체 검출을 위한 스크립트는 pytorch/vision/references/detection 의 transform.py 코드를 참고\n",
        "\n"
      ],
      "metadata": {
        "id": "XPVHZWyglM2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 준비"
      ],
      "metadata": {
        "id": "BnePaR3emZDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Faster R-CNN 모델의 백본으로 사용하려는 VGG-16 모델과 영역 제안 네트워크, 관심 영역 풀링을 적용한다."
      ],
      "metadata": {
        "id": "kf9Uoyw6mbVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 백본 및 모델 구조 정의"
      ],
      "metadata": {
        "id": "OEjRjRGKHg74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "from torchvision import ops\n",
        "from torchvision.models.detection import rpn\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "\n",
        "# vgg를 백본으로(마지막 분류 계층은 제외해 특징 추출 모델로 사용)\n",
        "backbone = models.vgg16(weights=\"VGG16_Weights.IMAGENET1K_V1\").features # features가 특징추출\n",
        "# 백본 모델은 출력 채널 수를 지정하는 out_channels 속성을 포함해야 한다\n",
        "backbone.out_channels = 512 # vgg-16은 512 채널 반환하므로 512로 할당\n",
        "\n",
        "# Faster R-CNN은 2-stage 객체 탐지 모델\n",
        "\n",
        "# 1. 영역 제한 네트워크 -> 입력 이미지에서 객체 위치 후보군을 생성하는 데 사용\n",
        "# 앵커 생성기(anchor_generator) 클래스는 객체 위치 후보군을 생성하는 데 사용된다.\n",
        "# 입력 이미지의 각 픽셀에 대해 앵커 박스를 생성한다.\n",
        "# 앵커 박스는 서로 다른 크기(sizes)와 종횡비(aspect_ratios)로 설정\n",
        "# 앵커 박스에 사용되는 매개변수의 형식은 Tuple[Tuple[int]] 구조를 가져야 한다.\n",
        "# **그러므로 콤마를 포함해 튜플 구조로 지정(,를 빼면 정수로 인식할 수 있음)**\n",
        "anchor_generator = rpn.AnchorGenerator(\n",
        "    sizes = ((32, 64, 128, 256, 512),),\n",
        "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
        ")\n",
        "\n",
        "# 2. 관심 영역 풀링 -> 객체 후보군을 입력으로 받아\n",
        "# 후보군 내의 특징 맵 영역을 일정한 크기의 고정된 영역으로 샘플링\n",
        "# 다중 스케일 관심 영역 정렬(MultiScaleRoIAlign) 클래스는\n",
        "# 관심 영역 정렬(ROI Align) 기능이 포함된 클래스로 다중 스케일 이미지에서 관심 영역 풀링 진행\n",
        "roi_pooler = ops.MultiScaleRoIAlign(\n",
        "    featmap_names=[\"0\"], # 특징맵 이름\n",
        "    # VGG-16 모델의 특징 추출 계층은 \"0\"으로 정의되어 있다.\n",
        "\n",
        "    output_size=(7,7),  # 관심 영역 풀링을 통해 추출된 특징맵 크기(H, W)\n",
        "    sampling_ratio=2 # 샘플링 비율\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = FasterRCNN(\n",
        "    backbone = backbone,\n",
        "    num_classes=3, # 배경도 클래스에 포함되므로 클래스 개수는 3으로 ?\n",
        "    rpn_anchor_generator=anchor_generator,\n",
        "    box_roi_pool=roi_pooler\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "9Mc738FKmZ5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7ccd8c-2893-4768-afac-2f6463f7a4c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:07<00:00, 69.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 최적화 함수 및 학습률 스케쥴러"
      ],
      "metadata": {
        "id": "z5sJkmp_z_wD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "# 모델 매개변수중 학습이 가능한 매개변수만 저장해서 경사하강법 적용\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# 학습률 스케줄러(지정된 주기마다 학습률을 감소시킨다)\n",
        "# 5 에폭(step_size)마다 학습률이 0.1(gamma) 씩 줄어든다.\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "# 스케줄러도 optimizer 변수처럼 step 메서드로 학습률을 갱신할 수 있다.\n",
        "# 일반적으로 한 에폭이 완료된 후에 호출"
      ],
      "metadata": {
        "id": "cfsmbUgTn-2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 학습"
      ],
      "metadata": {
        "id": "fH409tdeHVdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Faster R-CNN 미세 조정"
      ],
      "metadata": {
        "id": "UFc7KjEBHa_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "    cost = 0.0\n",
        "    for idx, (images, targets) in enumerate(train_dataloader):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        # 딕셔너리 간소화를 통해 적용\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # loss dict는 loss_classifier, loss_box_reg,\n",
        "        # 객체유무손실(loss_objectness), 영역 제안 네트워크 손실(loss_rpn_box_reg)\n",
        "        # 로 아뤄져 있다.\n",
        "        # Faster R-CNN은 학습모드일 때 모든 손실값을 출력한다.\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        # 네개의 손실이 모두 최소가 되는 방향으로 학습해야 하므로\n",
        "        # 손실값을 모두 더해 역전파를 계산한다.\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cost += losses\n",
        "\n",
        "    lr_scheduler.step() # 스케줄러 적용\n",
        "    cost = cost / len(train_dataloader)\n",
        "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj-VuyyFHVIt",
        "outputId": "786c9996-fee7-4355-9517-499108ed3032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch :    1, Cost : 0.444\n",
            "Epoch :    2, Cost : 0.306\n",
            "Epoch :    3, Cost : 0.281\n",
            "Epoch :    4, Cost : 0.268\n",
            "Epoch :    5, Cost : 0.255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 추론 및 시각화"
      ],
      "metadata": {
        "id": "yyOn_dtrNkUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "# Pillow 라이브러리로 사각형과 텍스트를 이미지 위에 그리는 함수\n",
        "def draw_bbox(ax, box, text, color):\n",
        "    ax.add_patch(\n",
        "        plt.Rectangle(\n",
        "            xy=(box[0], box[1]),\n",
        "            width=box[2] - box[0],\n",
        "            height=box[3] - box[1],\n",
        "            fill=False,\n",
        "            edgecolor=color,\n",
        "            linewidth=2,\n",
        "        )\n",
        "    )\n",
        "    ax.annotate(\n",
        "        text=text,\n",
        "        xy=(box[0] - 5, box[1] - 5),\n",
        "        color=color,\n",
        "        weight=\"bold\",\n",
        "        fontsize=13,\n",
        "    )\n",
        "\n",
        "threshold = 0.5\n",
        "categories = test_dataset.categories\n",
        "with torch.no_grad():\n",
        "    model.eval() # 평가모드\n",
        "    for images, targets in test_dataloader:\n",
        "        images = [image.to(device) for image in images]\n",
        "        outputs = model(images) # 추론\n",
        "\n",
        "        # 하나의 출력만 봄 (3개의 정보가 담겨있음)\n",
        "        boxes = outputs[0][\"boxes\"].to(\"cpu\").numpy()\n",
        "        labels = outputs[0][\"labels\"].to(\"cpu\").numpy()\n",
        "        scores = outputs[0][\"scores\"].to(\"cpu\").numpy()\n",
        "\n",
        "        # 임계값보다 높은 애들을 고름\n",
        "        boxes = boxes[scores >= threshold].astype(np.int32)\n",
        "        labels = labels[scores >= threshold]\n",
        "        scores = scores[scores >= threshold]\n",
        "\n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        ax = fig.add_subplot(1, 1, 1)\n",
        "        plt.imshow(to_pil_image(images[0]))\n",
        "\n",
        "        for box, label, score in zip(boxes, labels, scores):\n",
        "            draw_bbox(ax, box, f\"{categories[label]} - {score:.4f}\", \"red\")\n",
        "\n",
        "        tboxes = targets[0][\"boxes\"].numpy()\n",
        "        tlabels = targets[0][\"labels\"].numpy()\n",
        "        for box, label in zip(tboxes, tlabels):\n",
        "            draw_bbox(ax, box, f\"{categories[label]}\", \"blue\")\n",
        "\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "y_IZpktHIca5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 셀에서 사진이 너무 많이 출력되어 출력 제거\n",
        "\n",
        "사진에서 개와 고양이 객체 검출 박스를 보여줌"
      ],
      "metadata": {
        "id": "MaWbBBZOj3na"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 평가"
      ],
      "metadata": {
        "id": "mpc6IVjONo67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pycocotools.cocoeval import COCOeval # 코코 평가 클래스\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    coco_detections = [] # 추출 결과들\n",
        "    for images, targets in test_dataloader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        outputs = model(images)\n",
        "\n",
        "        for i in range(len(targets)):\n",
        "            image_id = targets[i][\"image_id\"].data.cpu().numpy().tolist()[0]\n",
        "            boxes = outputs[i][\"boxes\"].data.cpu().numpy()\n",
        "            boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
        "            boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
        "            scores = outputs[i][\"scores\"].data.cpu().numpy()\n",
        "            labels = outputs[i][\"labels\"].data.cpu().numpy()\n",
        "\n",
        "            # 임곗값을 두지 않고 모든 추출 결과를 저장\n",
        "            for instance_id in range(len(boxes)):\n",
        "                box = boxes[instance_id, :].tolist()\n",
        "\n",
        "                # 추출 결과는 [이미지 id, X, Y, W, H, 점수, label] 구조로 저장\n",
        "                prediction = np.array(\n",
        "                    [\n",
        "                        image_id,\n",
        "                        box[0],\n",
        "                        box[1],\n",
        "                        box[2],\n",
        "                        box[3],\n",
        "                        float(scores[instance_id]),\n",
        "                        int(labels[instance_id]),\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "                coco_detections.append(prediction)\n",
        "\n",
        "    coco_detections = np.asarray(coco_detections)\n",
        "    # coco dataset 실젯값 API\n",
        "    coco_gt = test_dataloader.dataset.coco\n",
        "    # 탐지된 결과\n",
        "    # loadRes로 COCO API 형식으로 변경\n",
        "    coco_dt = coco_gt.loadRes(coco_detections)\n",
        "    # COCOeval클래스로 평균 정밀도 계산\n",
        "    coco_evaluator = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
        "    # 정밀도, 재현율 계산\n",
        "    coco_evaluator.evaluate()\n",
        "    # evaluate 메서드로 계산한 평균 정밀도(Average Precision, AP)와\n",
        "    # 평균 재현율(Average recall) 누적\n",
        "    coco_evaluator.accumulate()\n",
        "    # 누적한 결과를 요약하고 출력\n",
        "    coco_evaluator.summarize()"
      ],
      "metadata": {
        "id": "brCbQICHNtKH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6356e02e-00cc-4aef-838b-d5179baf3bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing results...\n",
            "Converting ndarray to lists...\n",
            "(954, 7)\n",
            "0/954\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.09s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.04s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.262\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.634\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.017\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.238\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.290\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.316\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.407\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.407\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.050\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.315\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력 결과의 AP는 IoU 임곗값이 할당돼 있다. 일반적으로 IoU=0.50:0.95 인 mAP(mean AP)를 성능 평가 척도로 사용한다.\n",
        "\n",
        "IoU=0.50인 경우를 AP50, IOU=0.75인 경우를 AP75라 한다."
      ],
      "metadata": {
        "id": "DlGDJ_lbSHvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "첫 번쨰 결과인\n",
        "\n",
        "Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.262\n",
        "\n",
        "는 IoU를 0.5에서 0.95까지 0.05씩 높이면서 측정한 결과로 모든 객체 크기와 최대 100개의 객체까지 계산한 **AP값을 의미**한다.  이값은 0.262로, ***모델의 성능이 낮은 편이라는 것을 의미한다.***"
      ],
      "metadata": {
        "id": "Fo5V2j8MgP6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "두 번째 측정 결과인\n",
        "\n",
        "Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.634\n",
        "\n",
        "는 IoU가 0.5일 때, 모든 객체 크기와 최대 100개의 객체까지 계산한 AP 값이다. 이 값은 0.634로, 모델이 객체를 검출하는 데 있어 어느 정도의 성능을 보인다는 의미이다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gUwa3mGegoaZ"
      }
    }
  ]
}